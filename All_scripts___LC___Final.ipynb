{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB+2nJiRuy9VyZvX0VklPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam916060/PhD_Thesis/blob/main/All_scripts___LC___Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFR + Defualt"
      ],
      "metadata": {
        "id": "ijSl9PhIJ9r9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_HtifHdJL8V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the RandomForestRegressor with default hyperparameters\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the model to your data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model for prediction\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_pred)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mape_test = mean_absolute_percentage_error(y_test, y_pred)\n",
        "medae_test = median_absolute_error(y_test, y_pred)\n",
        "evs_test = explained_variance_score(y_test, y_pred)\n",
        "cv_test = np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test)\n",
        "\n",
        "print(\"Test set performance:\")\n",
        "print('R2 score:', r2_test)\n",
        "print('MAE:', mae_test)\n",
        "print('MSE:', mse_test)\n",
        "print('RMSE:', rmse_test)\n",
        "print('MAPE:', mape_test)\n",
        "print('MedAE:', medae_test)\n",
        "print('EVS:', evs_test)\n",
        "print('CV:', cv_test)\n",
        "\n",
        "best_model = model\n",
        "# Make predictions on training and test sets\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "# Evaluate the model on training set\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "print(\"Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train))\n",
        "print(\"MSE: {:.3f}\".format(mse_train))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train))\n",
        "print(\"R2: {:.3f}\".format(r2_train))\n",
        "\n",
        "# Evaluate the model on test set\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nTest set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_test))\n",
        "print(\"MSE: {:.3f}\".format(mse_test))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_test))\n",
        "print(\"R2: {:.3f}\".format(r2_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB + Default"
      ],
      "metadata": {
        "id": "3ZQNysw5KJnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor  # Import XGBRegressor instead of RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the XGBRegressor with default hyperparameters\n",
        "model_XGB = XGBRegressor(random_state=0)\n",
        "\n",
        "# Fit the XGBoost model to your data\n",
        "model_XGB.fit(X_train, y_train)\n",
        "\n",
        "# Use the XGBoost model for prediction\n",
        "y_pred_XGB = model_XGB.predict(X_test)\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "r2_test_XGB = r2_score(y_test, y_pred_XGB)\n",
        "mae_test_XGB = mean_absolute_error(y_test, y_pred_XGB)\n",
        "mse_test_XGB = mean_squared_error(y_test, y_pred_XGB)\n",
        "rmse_test_XGB = np.sqrt(mse_test_XGB)\n",
        "mape_test_XGB = mean_absolute_percentage_error(y_test, y_pred_XGB)\n",
        "medae_test_XGB = median_absolute_error(y_test, y_pred_XGB)\n",
        "evs_test_XGB = explained_variance_score(y_test, y_pred_XGB)\n",
        "cv_test_XGB = np.sqrt(mean_squared_error(y_test, y_pred_XGB)) / np.mean(y_test)\n",
        "\n",
        "print(\"XGBoost Test set performance:\")\n",
        "print('R2 score:', r2_test_XGB)\n",
        "print('MAE:', mae_test_XGB)\n",
        "print('MSE:', mse_test_XGB)\n",
        "print('RMSE:', rmse_test_XGB)\n",
        "print('MAPE:', mape_test_XGB)\n",
        "print('MedAE:', medae_test_XGB)\n",
        "print('EVS:', evs_test_XGB)\n",
        "print('CV:', cv_test_XGB)\n"
      ],
      "metadata": {
        "id": "ekA59hBSKNKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM + Default"
      ],
      "metadata": {
        "id": "MTp5lpXmKUhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the SVR with default hyperparameters\n",
        "model_SVM = SVR(C = 100, epsilon = 0.8)\n",
        "\n",
        "# Fit the model to your data\n",
        "model_SVM.fit(X_train, y_train)\n",
        "\n",
        "# Use the model for prediction\n",
        "y_pred_SVM = model_SVM.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "r2_test_SVM = r2_score(y_test, y_pred_SVM)\n",
        "mae_test_SVM = mean_absolute_error(y_test, y_pred_SVM)\n",
        "mse_test_SVM = mean_squared_error(y_test, y_pred_SVM)\n",
        "rmse_test_SVM = np.sqrt(mse_test_SVM)\n",
        "mape_test_SVM = mean_absolute_percentage_error(y_test, y_pred_SVM)\n",
        "medae_test_SVM = median_absolute_error(y_test, y_pred_SVM)\n",
        "evs_test_SVM = explained_variance_score(y_test, y_pred_SVM)\n",
        "cv_test_SVM = np.sqrt(mean_squared_error(y_test, y_pred_SVM)) / np.mean(y_test)\n",
        "\n",
        "print(\"SVM Test set performance:\")\n",
        "print('R2 score:', r2_test_SVM)\n",
        "print('MAE:', mae_test_SVM)\n",
        "print('MSE:', mse_test_SVM)\n",
        "print('RMSE:', rmse_test_SVM)\n",
        "print('MAPE:', mape_test_SVM)\n",
        "print('MedAE:', medae_test_SVM)\n",
        "print('EVS:', evs_test_SVM)\n",
        "print('CV:', cv_test_SVM)\n",
        "\n",
        "# Additional evaluation on the training set\n",
        "y_train_pred_SVM = model_SVM.predict(X_train)\n",
        "mae_train_SVM = mean_absolute_error(y_train, y_train_pred_SVM)\n",
        "mse_train_SVM = mean_squared_error(y_train, y_train_pred_SVM)\n",
        "rmse_train_SVM = np.sqrt(mse_train_SVM)\n",
        "r2_train_SVM = r2_score(y_train, y_train_pred_SVM)\n",
        "\n",
        "print(\"\\nSVM Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train_SVM))\n",
        "print(\"MSE: {:.3f}\".format(mse_train_SVM))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train_SVM))\n",
        "print(\"R2: {:.3f}\".format(r2_train_SVM))\n",
        "\n",
        "# Print header for SVM\n",
        "print(\"Training (SVM)\\t\\t\\tTesting (SVM)\")\n",
        "print(\"R2\\tMAE\\tMSE\\tRMSE\\tR2\\tMAE\\tMSE\\tRMSE\")\n",
        "\n",
        "# Print training set results for SVM\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\".format(r2_train_SVM, mae_train_SVM, mse_train_SVM, rmse_train_SVM), end='')\n",
        "\n",
        "# Print testing set results for SVM\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(r2_test_SVM, mae_test_SVM, mse_test_SVM, rmse_test_SVM))\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_test, y_pred_SVM,  marker='o', s=40, c=y_pred_SVM, cmap='viridis')\n",
        "ax.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='gray')\n",
        "ax.text(0, 140, r\"$R^2$ = %.2f\" % (r2_test_SVM), fontsize=12, bbox={\n",
        "        'facecolor': 'b', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_train, y_train_pred_SVM,  marker='D', s=40, c=y_train_pred_SVM, cmap='viridis')\n",
        "ax.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], '--', color='gray')\n",
        "ax.text(0, 180, r\"$R^2$ = %.2f\" % (r2_train_SVM), fontsize=12, bbox={\n",
        "        'facecolor': 'g', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mfme37hPKZO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFR + RGS"
      ],
      "metadata": {
        "id": "8wlZo7f8KhJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the RandomForestRegressor\n",
        "model = RandomForestRegressor(random_state=0)\n",
        "\n",
        "# Define the parameter grid for the random search\n",
        "param_dist = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=2000, num=10)],\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
        "    'max_depth': [int(x) for x in np.linspace(1, 110, num=5)],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Add default values for one iteration\n",
        "param_dist['n_estimators'].append(100)\n",
        "param_dist['max_features'].append(None)\n",
        "param_dist['max_depth'].append(None)\n",
        "param_dist['min_samples_split'].append(2)\n",
        "param_dist['min_samples_leaf'].append(1)\n",
        "\n",
        "# Perform the random search\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=51,  # 50 iterations for the randomized search + 1 iteration for default values\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "# Fit the random search to the data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the random search\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Use the best model for prediction\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_pred)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mape_test = mean_absolute_percentage_error(y_test, y_pred)\n",
        "medae_test = median_absolute_error(y_test, y_pred)\n",
        "evs_test = explained_variance_score(y_test, y_pred)\n",
        "cv_test = np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test)\n",
        "\n",
        "print(\"Test set performance:\")\n",
        "print('R2 score:', r2_test)\n",
        "print('MAE:', mae_test)\n",
        "print('MSE:', mse_test)\n",
        "print('RMSE:', rmse_test)\n",
        "print('MAPE:', mape_test)\n",
        "print('MedAE:', medae_test)\n",
        "print('EVS:', evs_test)\n",
        "print('CV:', cv_test)\n",
        "\n",
        "# Additional evaluation on the training set\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "print(\"\\nTraining set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train))\n",
        "print(\"MSE: {:.3f}\".format(mse_train))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train))\n",
        "print(\"R2: {:.3f}\".format(r2_train))\n"
      ],
      "metadata": {
        "id": "Gn1iyzhFKsCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB + RGS"
      ],
      "metadata": {
        "id": "sMEzfDMvK0Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 2)\n",
        "\n",
        "# Create the XGBRegressor\n",
        "model_XGB = XGBRegressor(random_state=0)\n",
        "\n",
        "# Define the parameter grid for the random search for XGBoost\n",
        "param_dist_XGB = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=2000, num=10)],\n",
        "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 1, 5],\n",
        "}\n",
        "\n",
        "# Perform the random search for XGBoost\n",
        "random_search_XGB = RandomizedSearchCV(\n",
        "    model_XGB,\n",
        "    param_distributions=param_dist_XGB,\n",
        "    n_iter=50,  # adjust the number of iterations as needed\n",
        "    cv=5,        # adjust the number of cross-validation folds as needed\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the random search to the data\n",
        "random_search_XGB.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the random search for XGBoost\n",
        "best_model_XGB = random_search_XGB.best_estimator_\n",
        "\n",
        "# Use the best model for prediction for XGBoost\n",
        "y_pred_XGB = best_model_XGB.predict(X_test)\n",
        "\n",
        "# Evaluate the best model for XGBoost\n",
        "r2_test_XGB = r2_score(y_test, y_pred_XGB)\n",
        "mae_test_XGB = mean_absolute_error(y_test, y_pred_XGB)\n",
        "mse_test_XGB = mean_squared_error(y_test, y_pred_XGB)\n",
        "rmse_test_XGB = np.sqrt(mse_test_XGB)\n",
        "mape_test_XGB = mean_absolute_percentage_error(y_test, y_pred_XGB)\n",
        "medae_test_XGB = median_absolute_error(y_test, y_pred_XGB)\n",
        "evs_test_XGB = explained_variance_score(y_test, y_pred_XGB)\n",
        "cv_test_XGB = np.sqrt(mean_squared_error(y_test, y_pred_XGB)) / np.mean(y_test)\n",
        "\n",
        "print(\"XGBoost Test set performance:\")\n",
        "print('R2 score:', r2_test_XGB)\n",
        "print('MAE:', mae_test_XGB)\n",
        "print('MSE:', mse_test_XGB)\n",
        "print('RMSE:', rmse_test_XGB)\n",
        "print('MAPE:', mape_test_XGB)\n",
        "print('MedAE:', medae_test_XGB)\n",
        "print('EVS:', evs_test_XGB)\n",
        "print('CV:', cv_test_XGB)\n",
        "\n",
        "# Rest of your code remains the same (plotting, training set evaluation, etc.)\n",
        "\n",
        "# Additional evaluation on training set for XGBoost\n",
        "y_train_pred_XGB = best_model_XGB.predict(X_train)\n",
        "mae_train_XGB = mean_absolute_error(y_train, y_train_pred_XGB)\n",
        "mse_train_XGB = mean_squared_error(y_train, y_train_pred_XGB)\n",
        "rmse_train_XGB = np.sqrt(mse_train_XGB)\n",
        "r2_train_XGB = r2_score(y_train, y_train_pred_XGB)\n",
        "\n",
        "print(\"\\nXGBoost Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train_XGB))\n",
        "print(\"MSE: {:.3f}\".format(mse_train_XGB))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train_XGB))\n",
        "print(\"R2: {:.3f}\".format(r2_train_XGB))\n",
        "\n",
        "# Get the best parameters from the random search for XGBoost\n",
        "best_params_XGB = random_search_XGB.best_params_\n",
        "\n",
        "print(\"Best hyperparameters for XGBoost:\")\n",
        "print(best_params_XGB)\n",
        "\n",
        "# Print header for XGBoost\n",
        "print(\"Training (XGBoost)\\t\\t\\tTesting (XGBoost)\")\n",
        "print(\"R2\\tMAE\\tMSE\\tRMSE\\tR2\\tMAE\\tMSE\\tRMSE\")\n",
        "\n",
        "# Print training set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\".format(r2_train_XGB, mae_train_XGB, mse_train_XGB, rmse_train_XGB), end='')\n",
        "\n",
        "# Print testing set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(r2_test_XGB, mae_test_XGB, mse_test_XGB, rmse_test_XGB))\n",
        "\n",
        "# Plotting for XGBoost\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_pred_XGB, y_test, marker='o', s=40, c=y_pred_XGB, cmap='viridis')\n",
        "ax.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='gray')\n",
        "ax.text(0, 140, r\"$R^2$ = %.2f\" % (r2_test_XGB), fontsize=12, bbox={\n",
        "        'facecolor': 'b', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_train_pred_XGB, y_train, marker='D', s=40, c=y_train_pred_XGB, cmap='viridis')\n",
        "ax.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], '--', color='gray')\n",
        "ax.text(0, 200, r\"$R^2$ = %.2f\" % (r2_train_XGB), fontsize=12, bbox={\n",
        "        'facecolor': 'g', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n"
      ],
      "metadata": {
        "id": "eRr-H1K_K264"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFR + BOT"
      ],
      "metadata": {
        "id": "twY7SZ1SLwHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the RandomForestRegressor with default parameters\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    min_weight_fraction_leaf=0.0,\n",
        "    max_features='auto',\n",
        "    max_leaf_nodes=None,\n",
        "    random_state=0,\n",
        "\n",
        ")\n",
        "\n",
        "# Define the parameter grid for Bayesian optimization\n",
        "param_dist = {\n",
        "    'n_estimators': (100, 3000),\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': (1, 100),\n",
        "    'min_samples_split': (2, 5, 10),\n",
        "    'min_samples_leaf': (1,2, 4)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "bayesian_search = BayesSearchCV(\n",
        "    model,\n",
        "    param_dist,\n",
        "    n_iter=50,  # adjust the number of iterations as needed\n",
        "    cv=5,       # adjust the number of cross-validation folds as needed\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the Bayesian search to the data\n",
        "bayesian_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from Bayesian optimization\n",
        "best_model = bayesian_search.best_estimator_\n",
        "\n",
        "# Use the best model for prediction\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_pred)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mape_test = mean_absolute_percentage_error(y_test, y_pred)\n",
        "medae_test = median_absolute_error(y_test, y_pred)\n",
        "evs_test = explained_variance_score(y_test, y_pred)\n",
        "cv_test = np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test)\n",
        "\n",
        "print(\"Test set performance:\")\n",
        "print('R2 score:', r2_test)\n",
        "print('MAE:', mae_test)\n",
        "print('MSE:', mse_test)\n",
        "print('RMSE:', rmse_test)\n",
        "print('MAPE:', mape_test)\n",
        "print('MedAE:', medae_test)\n",
        "print('EVS:', evs_test)\n",
        "print('CV:', cv_test)\n",
        "\n",
        "# Rest of your code remains the same (plotting, training set evaluation, etc.)\n",
        "\n",
        "# Additional evaluation on training set\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "print(\"\\nTraining set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train))\n",
        "print(\"MSE: {:.3f}\".format(mse_train))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train))\n",
        "print(\"R2: {:.3f}\".format(r2_train))\n"
      ],
      "metadata": {
        "id": "YWZtAHjVLu1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB + BOT"
      ],
      "metadata": {
        "id": "deC6kHd3Ly3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Create the XGBRegressor\n",
        "model_XGB = XGBRegressor(random_state=0)\n",
        "\n",
        "# Define the parameter grid for Bayesian optimization for XGBoost\n",
        "param_dist_XGB = {\n",
        "    'n_estimators': (100, 2000),\n",
        "    'learning_rate': (0.001, 0.01, 0.1, 0.2, 0.3),\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 1, 5],\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization for XGBoost\n",
        "bayesian_search_XGB = BayesSearchCV(\n",
        "    model_XGB,\n",
        "    param_dist_XGB,\n",
        "    n_iter=100,  # adjust the number of iterations as needed\n",
        "    cv=5,        # adjust the number of cross-validation folds as needed\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the Bayesian search to the data\n",
        "bayesian_search_XGB.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from Bayesian optimization for XGBoost\n",
        "best_model_XGB = bayesian_search_XGB.best_estimator_\n",
        "\n",
        "# Use the best model for prediction for XGBoost\n",
        "y_pred_XGB = best_model_XGB.predict(X_test)\n",
        "\n",
        "# Evaluate the best model for XGBoost\n",
        "r2_test_XGB = r2_score(y_test, y_pred_XGB)\n",
        "mae_test_XGB = mean_absolute_error(y_test, y_pred_XGB)\n",
        "mse_test_XGB = mean_squared_error(y_test, y_pred_XGB)\n",
        "rmse_test_XGB = np.sqrt(mse_test_XGB)\n",
        "mape_test_XGB = mean_absolute_percentage_error(y_test, y_pred_XGB)\n",
        "medae_test_XGB = median_absolute_error(y_test, y_pred_XGB)\n",
        "evs_test_XGB = explained_variance_score(y_test, y_pred_XGB)\n",
        "cv_test_XGB = np.sqrt(mean_squared_error(y_test, y_pred_XGB)) / np.mean(y_test)\n",
        "\n",
        "print(\"XGBoost Test set performance:\")\n",
        "print('R2 score:', r2_test_XGB)\n",
        "print('MAE:', mae_test_XGB)\n",
        "print('MSE:', mse_test_XGB)\n",
        "print('RMSE:', rmse_test_XGB)\n",
        "print('MAPE:', mape_test_XGB)\n",
        "print('MedAE:', medae_test_XGB)\n",
        "print('EVS:', evs_test_XGB)\n",
        "print('CV:', cv_test_XGB)\n",
        "\n",
        "# Additional evaluation on training set for XGBoost\n",
        "y_train_pred_XGB = best_model_XGB.predict(X_train)\n",
        "mae_train_XGB = mean_absolute_error(y_train, y_train_pred_XGB)\n",
        "mse_train_XGB = mean_squared_error(y_train, y_train_pred_XGB)\n",
        "rmse_train_XGB = np.sqrt(mse_train_XGB)\n",
        "r2_train_XGB = r2_score(y_train, y_train_pred_XGB)\n",
        "\n",
        "print(\"\\nXGBoost Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train_XGB))\n",
        "print(\"MSE: {:.3f}\".format(mse_train_XGB))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train_XGB))\n",
        "print(\"R2: {:.3f}\".format(r2_train_XGB))\n",
        "\n",
        "# Get the best parameters from the Bayesian search for XGBoost\n",
        "best_params_XGB = bayesian_search_XGB.best_params_\n",
        "\n",
        "print(\"Best hyperparameters for XGBoost:\")\n",
        "print(best_params_XGB)\n",
        "\n",
        "# Print header for XGBoost\n",
        "print(\"Training (XGBoost)\\t\\t\\tTesting (XGBoost)\")\n",
        "print(\"R2\\tMAE\\tMSE\\tRMSE\\tR2\\tMAE\\tMSE\\tRMSE\")\n",
        "\n",
        "# Print training set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\".format(r2_train_XGB, mae_train_XGB, mse_train_XGB, rmse_train_XGB), end='')\n",
        "\n",
        "# Print testing set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(r2_test_XGB, mae_test_XGB, mse_test_XGB, rmse_test_XGB))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JUOgJpCvL0fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFR + Optuna"
      ],
      "metadata": {
        "id": "TbHIcYmgMsSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "import optuna\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the parameter space for Optuna\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n",
        "    }\n",
        "\n",
        "    # Create the RandomForestRegressor with the suggested parameters\n",
        "    model = RandomForestRegressor(**params, random_state=0)\n",
        "\n",
        "    # Fit the model to your data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Use the model for prediction\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    return mse\n",
        "\n",
        "# Perform Optuna optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)  # adjust the number of trials as needed\n",
        "\n",
        "# Get the best parameters from the optimization\n",
        "best_params = study.best_params\n",
        "\n",
        "# Create the best model with the optimal parameters\n",
        "best_model = RandomForestRegressor(**best_params, random_state=0)\n",
        "\n",
        "# Fit the best model to your data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Use the best model for prediction\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "r2_test = r2_score(y_test, y_pred)\n",
        "mae_test = mean_absolute_error(y_test, y_pred)\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "mape_test = mean_absolute_percentage_error(y_test, y_pred)\n",
        "medae_test = median_absolute_error(y_test, y_pred)\n",
        "evs_test = explained_variance_score(y_test, y_pred)\n",
        "cv_test = np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test)\n",
        "\n",
        "print(\"Test set performance:\")\n",
        "print('R2 score:', r2_test)\n",
        "print('MAE:', mae_test)\n",
        "print('MSE:', mse_test)\n",
        "print('RMSE:', rmse_test)\n",
        "print('MAPE:', mape_test)\n",
        "print('MedAE:', medae_test)\n",
        "print('EVS:', evs_test)\n",
        "print('CV:', cv_test)\n",
        "\n",
        "# Additional evaluation on the training set\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "print(\"\\nTraining set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train))\n",
        "print(\"MSE: {:.3f}\".format(mse_train))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train))\n",
        "print(\"R2: {:.3f}\".format(r2_train))\n"
      ],
      "metadata": {
        "id": "5ZrUxDinMvJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB + Optuna"
      ],
      "metadata": {
        "id": "rM5hENu6M-SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "import optuna\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the parameter space for Optuna for XGBoost\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 20),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.8, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.8, 1.0),\n",
        "        'gamma': trial.suggest_uniform('gamma', 0, 5),\n",
        "    }\n",
        "\n",
        "    # Create the XGBRegressor with the suggested parameters\n",
        "    model_XGB = XGBRegressor(**params, random_state=0)\n",
        "\n",
        "    # Fit the model to your data\n",
        "    model_XGB.fit(X_train, y_train)\n",
        "\n",
        "    # Use the model for prediction\n",
        "    y_pred_XGB = model_XGB.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse_XGB = mean_squared_error(y_test, y_pred_XGB)\n",
        "\n",
        "    return mse_XGB\n",
        "\n",
        "# Perform Optuna optimization for XGBoost\n",
        "study_XGB = optuna.create_study(direction='minimize')\n",
        "study_XGB.optimize(objective, n_trials=100)  # adjust the number of trials as needed\n",
        "\n",
        "# Get the best parameters from the optimization for XGBoost\n",
        "best_params_XGB = study_XGB.best_params\n",
        "\n",
        "# Create the best model with the optimal parameters for XGBoost\n",
        "best_model_XGB = XGBRegressor(**best_params_XGB, random_state=0)\n",
        "\n",
        "# Fit the best model to your data for XGBoost\n",
        "best_model_XGB.fit(X_train, y_train)\n",
        "\n",
        "# Use the best model for prediction for XGBoost\n",
        "y_pred_XGB = best_model_XGB.predict(X_test)\n",
        "\n",
        "# Evaluate the best model for XGBoost\n",
        "r2_test_XGB = r2_score(y_test, y_pred_XGB)\n",
        "mae_test_XGB = mean_absolute_error(y_test, y_pred_XGB)\n",
        "mse_test_XGB = mean_squared_error(y_test, y_pred_XGB)\n",
        "rmse_test_XGB = np.sqrt(mse_test_XGB)\n",
        "mape_test_XGB = mean_absolute_percentage_error(y_test, y_pred_XGB)\n",
        "medae_test_XGB = median_absolute_error(y_test, y_pred_XGB)\n",
        "evs_test_XGB = explained_variance_score(y_test, y_pred_XGB)\n",
        "cv_test_XGB = np.sqrt(mean_squared_error(y_test, y_pred_XGB)) / np.mean(y_test)\n",
        "\n",
        "print(\"XGBoost Test set performance:\")\n",
        "print('R2 score:', r2_test_XGB)\n",
        "print('MAE:', mae_test_XGB)\n",
        "print('MSE:', mse_test_XGB)\n",
        "print('RMSE:', rmse_test_XGB)\n",
        "print('MAPE:', mape_test_XGB)\n",
        "print('MedAE:', medae_test_XGB)\n",
        "print('EVS:', evs_test_XGB)\n",
        "print('CV:', cv_test_XGB)\n",
        "\n",
        "# Additional evaluation on the training set for XGBoost\n",
        "y_train_pred_XGB = best_model_XGB.predict(X_train)\n",
        "mae_train_XGB = mean_absolute_error(y_train, y_train_pred_XGB)\n",
        "mse_train_XGB = mean_squared_error(y_train, y_train_pred_XGB)\n",
        "rmse_train_XGB = np.sqrt(mse_train_XGB)\n",
        "r2_train_XGB = r2_score(y_train, y_train_pred_XGB)\n",
        "\n",
        "print(\"\\nXGBoost Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train_XGB))\n",
        "print(\"MSE: {:.3f}\".format(mse_train_XGB))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train_XGB))\n",
        "print(\"R2: {:.3f}\".format(r2_train_XGB))\n",
        "\n",
        "# Get the best parameters from the optimization for XGBoost\n",
        "best_params_XGB = study_XGB.best_params\n",
        "\n",
        "print(\"Best hyperparameters for XGBoost:\")\n",
        "print(best_params_XGB)\n",
        "\n",
        "# Print header for XGBoost\n",
        "print(\"Training (XGBoost)\\t\\t\\tTesting (XGBoost)\")\n",
        "print(\"R2\\tMAE\\tMSE\\tRMSE\\tR2\\tMAE\\tMSE\\tRMSE\")\n",
        "\n",
        "# Print training set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\".format(r2_train_XGB, mae_train_XGB, mse_train_XGB, rmse_train_XGB), end='')\n",
        "\n",
        "# Print testing set results for XGBoost\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(r2_test_XGB, mae_test_XGB, mse_test_XGB, rmse_test_XGB))\n",
        "\n",
        "# Plotting for XGBoost\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_pred_XGB, y_test, marker='o', s=40, c=y_pred_XGB, cmap='viridis')\n",
        "ax.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='gray')\n",
        "ax.text(0, 140, r\"$R^2$ = %.2f\" % (r2_test_XGB), fontsize=12, bbox={\n",
        "        'facecolor': 'b', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "sc = ax.scatter(y_train_pred_XGB, y_train, marker='D', s=40, c=y_train_pred_XGB, cmap='viridis')\n",
        "ax.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], '--', color='gray')\n",
        "ax.text(0, 180, r\"$R^2$ = %.2f\" % (r2_train_XGB), fontsize=12, bbox={\n",
        "        'facecolor': 'g', 'alpha': 0.5, 'pad': 10})\n",
        "ax.set_ylabel('Predicted Cracking (m)', size='14')\n",
        "ax.set_xlabel('Actual Cracking (m)', size='14')\n",
        "\n",
        "cbar = fig.colorbar(sc, shrink=1)\n"
      ],
      "metadata": {
        "id": "DFXcmhLWNACp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN Basic"
      ],
      "metadata": {
        "id": "OZfhzx0NNZnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_28-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "# Create the DNN model\n",
        "model_DNN = Sequential()\n",
        "model_DNN.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model_DNN.add(Dense(64, activation='relu'))\n",
        "model_DNN.add(Dense(64, activation='relu'))\n",
        "model_DNN.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model_DNN.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the model to your data\n",
        "model_DNN.fit(X_train, y_train, epochs=300, batch_size=32, verbose=1)\n",
        "\n",
        "# Use the model for prediction\n",
        "y_pred_DNN = model_DNN.predict(X_test).flatten()\n",
        "\n",
        "# Evaluate the model\n",
        "r2_test_DNN = r2_score(y_test, y_pred_DNN)\n",
        "mae_test_DNN = mean_absolute_error(y_test, y_pred_DNN)\n",
        "mse_test_DNN = mean_squared_error(y_test, y_pred_DNN)\n",
        "rmse_test_DNN = np.sqrt(mse_test_DNN)\n",
        "mape_test_DNN = mean_absolute_percentage_error(y_test, y_pred_DNN)\n",
        "medae_test_DNN = median_absolute_error(y_test, y_pred_DNN)\n",
        "evs_test_DNN = explained_variance_score(y_test, y_pred_DNN)\n",
        "cv_test_DNN = np.sqrt(mean_squared_error(y_test, y_pred_DNN)) / np.mean(y_test)\n",
        "\n",
        "print(\"DNN Test set performance:\")\n",
        "print('R2 score:', r2_test_DNN)\n",
        "print('MAE:', mae_test_DNN)\n",
        "print('MSE:', mse_test_DNN)\n",
        "print('RMSE:', rmse_test_DNN)\n",
        "print('MAPE:', mape_test_DNN)\n",
        "print('MedAE:', medae_test_DNN)\n",
        "print('EVS:', evs_test_DNN)\n",
        "print('CV:', cv_test_DNN)\n",
        "\n",
        "# Additional evaluation on the training set\n",
        "y_train_pred_DNN = model_DNN.predict(X_train).flatten()\n",
        "mae_train_DNN = mean_absolute_error(y_train, y_train_pred_DNN)\n",
        "mse_train_DNN = mean_squared_error(y_train, y_train_pred_DNN)\n",
        "rmse_train_DNN = np.sqrt(mse_train_DNN)\n",
        "r2_train_DNN = r2_score(y_train, y_train_pred_DNN)\n",
        "\n",
        "print(\"\\nDNN Training set performance:\")\n",
        "print(\"MAE: {:.3f}\".format(mae_train_DNN))\n",
        "print(\"MSE: {:.3f}\".format(mse_train_DNN))\n",
        "print(\"RMSE: {:.3f}\".format(rmse_train_DNN))\n",
        "print(\"R2: {:.3f}\".format(r2_train_DNN))\n",
        "\n",
        "# Print header for DNN\n",
        "print(\"\\nTraining (DNN)\\t\\t\\tTesting (DNN)\")\n",
        "print(\"R2\\tMAE\\tMSE\\tRMSE\\tR2\\tMAE\\tMSE\\tRMSE\")\n",
        "\n",
        "# Print training set results for DNN\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\".format(r2_train_DNN, mae_train_DNN, mse_train_DNN, rmse_train_DNN), end='')\n",
        "\n",
        "# Print testing set results for DNN\n",
        "print(\"{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(r2_test_DNN, mae_test_DNN, mse_test_DNN, rmse_test_DNN))\n"
      ],
      "metadata": {
        "id": "NttL45oUNu7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN + Keras"
      ],
      "metadata": {
        "id": "3BaXJBncON0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    median_absolute_error,\n",
        "    explained_variance_score,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_28-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
        "    for i in range(hp.Int('num_layers', 1, 5)):\n",
        "        model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=512, step=32), activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Define the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='dnn_optimization'\n",
        ")\n",
        "\n",
        "# Search for the best hyperparameter configuration\n",
        "tuner.search(X_train, y_train, epochs=200, validation_data=(X_test, y_test))\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Print a summary of the best model\n",
        "best_model.summary()\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred_DNN = best_model.predict(X_test).flatten()\n",
        "\n",
        "# Evaluate the model\n",
        "r2_test_DNN = r2_score(y_test, y_pred_DNN)\n",
        "mae_test_DNN = mean_absolute_error(y_test, y_pred_DNN)\n",
        "mse_test_DNN = mean_squared_error(y_test, y_pred_DNN)\n",
        "rmse_test_DNN = np.sqrt(mse_test_DNN)\n",
        "mape_test_DNN = mean_absolute_percentage_error(y_test, y_pred_DNN)\n",
        "medae_test_DNN = median_absolute_error(y_test, y_pred_DNN)\n",
        "evs_test_DNN = explained_variance_score(y_test, y_pred_DNN)\n",
        "cv_test_DNN = np.sqrt(mean_squared_error(y_test, y_pred_DNN)) / np.mean(y_test)\n",
        "\n",
        "print(\"DNN Test set performance:\")\n",
        "print('R2 score:', r2_test_DNN)\n",
        "print('MAE:', mae_test_DNN)\n",
        "print('MSE:', mse_test_DNN)\n",
        "print('RMSE:', rmse_test_DNN)\n",
        "print('MAPE:', mape_test_DNN)\n",
        "print('MedAE:', medae_test_DNN)\n",
        "print('EVS:', evs_test_DNN)\n",
        "print('CV:', cv_test_DNN)\n"
      ],
      "metadata": {
        "id": "fb2jlM-4OPaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN + Optuna"
      ],
      "metadata": {
        "id": "3_GuiPxXOd7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('LC_29-11-23.csv')\n",
        "\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter space\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
        "    num_nodes = trial.suggest_int('num_nodes', 32, 512)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "\n",
        "    # Create the DNN model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_nodes, input_dim=X_train.shape[1], activation='relu'))\n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Dense(num_nodes, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "\n",
        "    # Fit the model to your data\n",
        "    model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
        "\n",
        "    # Use the model for prediction on training set\n",
        "    y_pred_train = model.predict(X_train).flatten()\n",
        "\n",
        "    # Evaluate the model on training set\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    return mse_train\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_num_layers = best_params['num_layers']\n",
        "best_num_nodes = best_params['num_nodes']\n",
        "best_learning_rate = best_params['learning_rate']\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Number of Layers:\", best_num_layers)\n",
        "print(\"Number of Nodes:\", best_num_nodes)\n",
        "print(\"Learning Rate:\", best_learning_rate)\n",
        "\n",
        "# Build the final model with the best hyperparameters\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(best_num_nodes, input_dim=X_train.shape[1], activation='relu'))\n",
        "for _ in range(best_num_layers - 1):\n",
        "    final_model.add(Dense(best_num_nodes, activation='relu'))\n",
        "final_model.add(Dense(1, activation='linear'))\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_learning_rate), loss='mean_squared_error')\n",
        "\n",
        "# Fit the final model to your data\n",
        "final_model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=1)\n",
        "\n",
        "# Use the final model for prediction on training set\n",
        "y_pred_train_final = final_model.predict(X_train).flatten()\n",
        "\n",
        "# Evaluate the final model on training set\n",
        "mse_train_final = mean_squared_error(y_train, y_pred_train_final)\n",
        "\n",
        "# Print the MSE for the final model on training set\n",
        "print(\"\\nFinal Model Training set performance (after optimization):\")\n",
        "print('Mean Squared Error (Training):', mse_train_final)\n"
      ],
      "metadata": {
        "id": "UAiywfHXOgRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}